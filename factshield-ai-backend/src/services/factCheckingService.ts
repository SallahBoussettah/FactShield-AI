import { HfInference } from '@huggingface/inference';
import { logger } from '../utils/logger';
import { ExtractedClaim } from './claimExtractionService';
import { sourceGenerationService } from './sourceGenerationService';
import { geminiService, GeminiAnalysisResult } from './geminiService';

export interface FactCheckSource {
  url: string;
  title: string;
  reliability: number;
  domain: string;
  publishDate?: string;
  author?: string;
  relevanceScore: number;
  factCheckResult: 'supports' | 'contradicts' | 'neutral' | 'insufficient_evidence';
  excerpt?: string;
}

export interface CredibilityAssessment {
  overallScore: number; // 0-1, overall credibility
  confidence: number; // 0-1, confidence in the assessment
  factors: {
    sourceReliability: number;
    evidenceStrength: number;
    consensusLevel: number;
    recency: number;
    authorCredibility: number;
  };
  reasoning: string;
  riskLevel: 'low' | 'medium' | 'high' | 'critical';
}

export interface FactCheckResult {
  claimId: string;
  originalClaim: string;
  credibilityScore: number;
  credibilityAssessment: CredibilityAssessment;
  sources: FactCheckSource[];
  verificationStatus: 'verified' | 'disputed' | 'unverified' | 'false';
  processingTime: number;
}

export interface FactCheckingOptions {
  maxSources?: number;
  minSourceReliability?: number;
  includeDisputedSources?: boolean;
  searchTimeout?: number;
  useRealTimeData?: boolean;
}

class FactCheckingService {
  private hf: HfInference;
  private readonly defaultOptions: Required<FactCheckingOptions> = {
    maxSources: 5,
    minSourceReliability: 0.6,
    includeDisputedSources: false,
    searchTimeout: 10000,
    useRealTimeData: true
  };

  // No hardcoded domains - all sources generated by Gemini AI

  // Models for different fact-checking tasks
  private readonly models = {
    textClassification: 'cardiffnlp/twitter-roberta-base-sentiment-latest',
    questionAnswering: 'deepset/roberta-base-squad2',
    zeroShotClassification: 'facebook/bart-large-mnli',
    textSimilarity: 'sentence-transformers/all-MiniLM-L6-v2'
  };

  constructor() {
    const apiKey = process.env.HUGGINGFACE_API_KEY;
    this.hf = new HfInference(apiKey);
  }

  /**
   * Main fact-checking method
   */
  async factCheck(claim: ExtractedClaim, options: FactCheckingOptions = {}, externalSources?: FactCheckSource[]): Promise<FactCheckResult> {
    const startTime = Date.now();
    const config = { ...this.defaultOptions, ...options };

    try {
      logger.info(`Starting fact-check for claim: ${claim.text.substring(0, 50)}...`);

      // Use external sources if provided, otherwise generate sources
      const sources = externalSources && externalSources.length > 0 
        ? externalSources 
        : await this.searchForSources(claim.text, config);

      // Assess credibility using AI
      const credibilityAssessment = await this.assessCredibility(claim.text, sources);

      // Determine verification status
      const verificationStatus = this.determineVerificationStatus(credibilityAssessment);

      const processingTime = Date.now() - startTime;

      const result: FactCheckResult = {
        claimId: claim.id,
        originalClaim: claim.text,
        credibilityScore: credibilityAssessment.overallScore,
        credibilityAssessment,
        sources,
        verificationStatus,
        processingTime
      };

      logger.info(`Fact-check completed for claim ${claim.id} in ${processingTime}ms`);
      return result;

    } catch (error) {
      logger.error(`Fact-check failed for claim ${claim.id}:`, error);
      throw error;
    }
  }

  /**
   * Search for sources using verified real sources
   */
  private async searchForSources(claim: string, options: FactCheckingOptions): Promise<FactCheckSource[]> {
    const config = { ...this.defaultOptions, ...options };

    // If maxSources is 0, don't generate sources for individual claims
    // Sources will be provided at the content level by the controller
    if (config.maxSources === 0) {
      logger.info('Skipping individual claim source generation - sources provided at content level');
      return [];
    }

    try {
      logger.info(`Searching for verified sources for claim: ${claim.substring(0, 50)}...`);

      // Use the improved source generation service with real verification
      const generatedSources = await sourceGenerationService.generateSources(claim, {
        maxSources: config.maxSources,
        minReliability: config.minSourceReliability,
        includeDisputedSources: config.includeDisputedSources
      });

      // Convert GeneratedSource to FactCheckSource format, only include verified sources
      const sources: FactCheckSource[] = generatedSources
        .filter(source => source.verificationStatus === 'verified')
        .map(source => ({
          url: source.url,
          title: source.title,
          reliability: source.reliability,
          domain: source.domain,
          publishDate: source.publishDate,
          author: source.author,
          relevanceScore: source.relevanceScore,
          factCheckResult: source.factCheckResult,
          excerpt: source.excerpt
        }));

      logger.info(`Found ${sources.length} verified sources for claim`);
      return sources;

    } catch (error) {
      logger.error('Source search failed:', error);
      return [];
    }
  }

  /**
   * Assess credibility using AI models and source analysis
   */
  private async assessCredibility(claim: string, sources: FactCheckSource[]): Promise<CredibilityAssessment> {
    try {
      // Use Gemini for better analysis if available
      let geminiAnalysis: GeminiAnalysisResult | null = null;

      if (geminiService.isAvailable()) {
        try {
          geminiAnalysis = await geminiService.analyzeClaim(claim);
          logger.info(`Gemini analysis: credibility=${geminiAnalysis.credibilityScore}, risk=${geminiAnalysis.riskLevel}`);
        } catch (error) {
          logger.warn('Gemini analysis failed, falling back to Hugging Face:', error);
        }
      }

      // If we have Gemini analysis, use it as the primary assessment
      if (geminiAnalysis) {
        const sourceReliability = sources.length > 0
          ? sources.reduce((sum, source) => sum + source.reliability, 0) / sources.length
          : 0.8;

        const evidenceStrength = this.calculateEvidenceStrength(sources);
        const consensusLevel = this.calculateConsensusLevel(sources);
        const recency = this.calculateRecencyScore(sources);
        const authorCredibility = this.calculateAuthorCredibility(sources);

        return {
          overallScore: geminiAnalysis.credibilityScore,
          confidence: geminiAnalysis.confidence,
          factors: {
            sourceReliability,
            evidenceStrength,
            consensusLevel,
            recency,
            authorCredibility
          },
          reasoning: geminiAnalysis.reasoning,
          riskLevel: geminiAnalysis.riskLevel
        };
      }

      // Fallback to Hugging Face analysis when Gemini is unavailable
      logger.warn('Gemini unavailable, using Hugging Face for basic analysis');

      // Use AI to analyze the claim for credibility indicators
      const credibilityResult = await this.hf.zeroShotClassification({
        model: this.models.zeroShotClassification,
        inputs: claim,
        parameters: {
          candidate_labels: [
            'scientifically accurate information',
            'potentially misleading claims',
            'dangerous misinformation',
            'unverified assertions',
            'conspiracy theory content'
          ]
        }
      }) as any;

      const credibilityIndicator = credibilityResult.labels?.[0] || 'unverified assertions';
      const aiConfidence = credibilityResult.scores?.[0] || 0.5;

      // Calculate source-based factors
      const sourceReliability = sources.length > 0
        ? sources.reduce((sum, source) => sum + source.reliability, 0) / sources.length
        : 0.5;

      const evidenceStrength = this.calculateEvidenceStrength(sources);
      const consensusLevel = this.calculateConsensusLevel(sources);
      const recency = this.calculateRecencyScore(sources);
      const authorCredibility = this.calculateAuthorCredibility(sources);

      // Calculate overall score
      const overallScore = this.calculateOverallCredibilityScore({
        sourceReliability,
        evidenceStrength,
        consensusLevel,
        recency,
        authorCredibility,
        aiConfidence
      });

      // Determine risk level
      const riskLevel = this.determineRiskLevel(credibilityIndicator, overallScore);

      // Generate reasoning
      const reasoning = this.generateCredibilityReasoning(
        credibilityIndicator,
        overallScore,
        sources.length,
        aiConfidence
      );

      return {
        overallScore,
        confidence: aiConfidence,
        factors: {
          sourceReliability,
          evidenceStrength,
          consensusLevel,
          recency,
          authorCredibility
        },
        reasoning,
        riskLevel
      };

    } catch (error) {
      logger.warn('AI credibility assessment failed:', error);
      return this.generateFallbackCredibilityAssessment(sources);
    }
  }

  /**
   * Calculate evidence strength based on source quality and consistency
   */
  private calculateEvidenceStrength(sources: FactCheckSource[]): number {
    if (sources.length === 0) return 0.3;

    const supportingSources = sources.filter(s => s.factCheckResult === 'supports').length;
    const contradictingSources = sources.filter(s => s.factCheckResult === 'contradicts').length;
    const neutralSources = sources.filter(s => s.factCheckResult === 'neutral').length;
    const insufficientSources = sources.filter(s => s.factCheckResult === 'insufficient_evidence').length;

    // If sources contradict the claim, evidence strength for the CLAIM is low
    if (contradictingSources > supportingSources + neutralSources) return 0.1; // Very low evidence for false claims
    if (supportingSources > contradictingSources + neutralSources) return 0.8; // High evidence for true claims
    if (insufficientSources > supportingSources + contradictingSources) return 0.2; // Low evidence for unverified claims

    // Moderate evidence if mixed
    return 0.5;
  }

  /**
   * Calculate consensus level among sources
   */
  private calculateConsensusLevel(sources: FactCheckSource[]): number {
    if (sources.length === 0) return 0.3;

    const results = sources.map(s => s.factCheckResult);
    const resultCounts = results.reduce((acc, result) => {
      acc[result] = (acc[result] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);

    const maxCount = Math.max(...Object.values(resultCounts));
    return maxCount / sources.length;
  }

  /**
   * Calculate recency score based on publication dates
   */
  private calculateRecencyScore(sources: FactCheckSource[]): number {
    if (sources.length === 0) return 0.5;

    const now = new Date();
    const scores = sources.map(source => {
      if (!source.publishDate) return 0.5;

      const publishDate = new Date(source.publishDate);
      const daysDiff = (now.getTime() - publishDate.getTime()) / (1000 * 60 * 60 * 24);

      // More recent = higher score
      if (daysDiff <= 30) return 1.0;
      if (daysDiff <= 90) return 0.8;
      if (daysDiff <= 365) return 0.6;
      return 0.4;
    });

    return scores.reduce((sum, score) => sum + score, 0) / scores.length;
  }

  /**
   * Calculate author credibility based on source reliability scores from Gemini
   */
  private calculateAuthorCredibility(sources: FactCheckSource[]): number {
    if (sources.length === 0) return 0.5;

    const credibilityScores = sources.map(source => source.reliability);
    return credibilityScores.reduce((sum, score) => sum + score, 0) / credibilityScores.length;
  }

  /**
   * Calculate overall credibility score
   */
  private calculateOverallCredibilityScore(factors: {
    sourceReliability: number;
    evidenceStrength: number;
    consensusLevel: number;
    recency: number;
    authorCredibility: number;
    aiConfidence: number;
  }): number {
    // Weighted average of all factors
    const weights = {
      sourceReliability: 0.25,
      evidenceStrength: 0.25,
      consensusLevel: 0.20,
      recency: 0.10,
      authorCredibility: 0.15,
      aiConfidence: 0.05
    };

    const baseScore = (
      factors.sourceReliability * weights.sourceReliability +
      factors.evidenceStrength * weights.evidenceStrength +
      factors.consensusLevel * weights.consensusLevel +
      factors.recency * weights.recency +
      factors.authorCredibility * weights.authorCredibility +
      factors.aiConfidence * weights.aiConfidence
    );

    return baseScore;
  }

  /**
   * Determine risk level based on AI analysis and credibility score
   */
  private determineRiskLevel(credibilityIndicator: string, overallScore: number): CredibilityAssessment['riskLevel'] {
    if (credibilityIndicator.includes('dangerous') || overallScore < 0.3) {
      return 'critical';
    } else if (credibilityIndicator.includes('misleading') || credibilityIndicator.includes('conspiracy') || overallScore < 0.5) {
      return 'high';
    } else if (credibilityIndicator.includes('accurate') && overallScore > 0.7) {
      return 'low';
    }
    return 'medium';
  }

  /**
   * Generate human-readable credibility reasoning
   */
  private generateCredibilityReasoning(
    credibilityIndicator: string,
    overallScore: number,
    sourceCount: number,
    aiConfidence: number
  ): string {
    const scoreDescription = overallScore > 0.7 ? 'high' : overallScore > 0.5 ? 'moderate' : 'low';
    const confidenceDescription = aiConfidence > 0.7 ? 'high' : aiConfidence > 0.5 ? 'moderate' : 'low';

    return `AI analysis classified this as ${credibilityIndicator} with ${confidenceDescription} confidence (${(aiConfidence * 100).toFixed(0)}%). ` +
      `Based on ${sourceCount} sources, the overall credibility score is ${scoreDescription} (${(overallScore * 100).toFixed(0)}%).`;
  }

  /**
   * Generate fallback credibility assessment when AI fails
   */
  private generateFallbackCredibilityAssessment(sources: FactCheckSource[]): CredibilityAssessment {
    const sourceReliability = sources.length > 0
      ? sources.reduce((sum, source) => sum + source.reliability, 0) / sources.length
      : 0.5;

    return {
      overallScore: sourceReliability,
      confidence: 0.5,
      factors: {
        sourceReliability,
        evidenceStrength: 0.5,
        consensusLevel: 0.5,
        recency: 0.5,
        authorCredibility: sourceReliability
      },
      reasoning: 'AI analysis unavailable. Assessment based on source reliability only.',
      riskLevel: 'medium'
    };
  }

  /**
   * Determine verification status based on credibility assessment
   */
  private determineVerificationStatus(assessment: CredibilityAssessment): FactCheckResult['verificationStatus'] {
    if (assessment.riskLevel === 'critical' || assessment.overallScore < 0.3) {
      return 'false';
    } else if (assessment.riskLevel === 'high' || assessment.overallScore < 0.5) {
      return 'disputed';
    } else if (assessment.overallScore > 0.7) {
      return 'verified';
    }
    return 'unverified';
  }



  /**
   * Batch fact-check multiple claims
   */
  async batchFactCheck(claims: ExtractedClaim[], options: FactCheckingOptions = {}, sharedSources?: FactCheckSource[]): Promise<FactCheckResult[]> {
    const results: FactCheckResult[] = [];

    for (const claim of claims) {
      try {
        const result = await this.factCheck(claim, options, sharedSources);
        results.push(result);

        // Add delay to avoid rate limiting
        await new Promise(resolve => setTimeout(resolve, 500));
      } catch (error) {
        logger.error(`Failed to fact-check claim ${claim.id}:`, error);
        // Continue with other claims
      }
    }

    return results;
  }

  /**
   * Get service health status
   */
  async getHealthStatus(): Promise<{ status: 'healthy' | 'degraded' | 'unhealthy'; details: any }> {
    try {
      // Test Gemini service
      const geminiHealth = await geminiService.getHealthStatus();

      // Test AI service
      const testResult = await this.hf.zeroShotClassification({
        model: this.models.zeroShotClassification,
        inputs: 'This is a test sentence.',
        parameters: {
          candidate_labels: ['test', 'example']
        }
      }) as any;

      // Test source generation service
      const testSources = await sourceGenerationService.generateSources('test claim', { maxSources: 1 });

      const overallStatus = geminiHealth.status === 'healthy' ? 'healthy' :
        geminiHealth.status === 'degraded' ? 'degraded' : 'unhealthy';

      return {
        status: overallStatus,
        details: {
          gemini: geminiHealth,
          huggingFace: testResult ? 'operational' : 'failed',
          sourceGeneration: testSources.length > 0 ? 'operational' : 'failed',
          geminiPowered: true
        }
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        details: {
          error: error instanceof Error ? error.message : 'Unknown error'
        }
      };
    }
  }
}

export const factCheckingService = new FactCheckingService();